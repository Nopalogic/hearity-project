# Data Pipeline Using Workflow Orchestration

The data pipeline in this workflow uses Google Cloud Platform (GCP) virtual machine (VM) and Apache Airflow to automate the process of extracting, transforming, and loading (ETL) data from a MongoDB server to MongoDB server again. This pipeline is designed to address the challenge of raw data being irregularly spaced, with a gap of six months between each data. Because the raw data cannot directly be fed into the forecasting model due to this infrequent update cycle, additional steps like upsampling are required to fill in the gaps and make the data more consistent and usable for accurate daily forecasting.

The core of this solution involves batch processing rather than real-time streaming. Since forecasting is expected only once per day, the batch approach efficiently handles the required transformations and data enrichment. Apache Airflow is utilized to orchestrate the workflow, scheduling tasks like data extraction from MongoDB, upsampling to adjust the temporal resolution, and applying the forecasting model. Once the transformed and forecasted data is ready, it is loaded back into the MongoDB server for further use. This structured batch processing ensures that the data is appropriately prepared and enriched before being passed to downstream applications. Finally, the results of the forecasting process are intended to be used by mobile developers for visualization purposes. By enriching the raw data and providing accurate forecasts, the pipeline ensures that the mobile application can display meaningful and up-to-date insights, allowing users to view the results of tests or other predictive analyses.